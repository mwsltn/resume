\documentclass[11pt]{article}
\usepackage{fancyhdr}

\usepackage[letterpaper,top=1in, bottom=1.5in, outer=1in, inner=1in]{geometry}
\usepackage[dvipsnames]{xcolor}
\usepackage{vwcol}  

\pagestyle{fancyplain}

\setlength\headheight{57pt}
\setlength\parindent{0pt}

\newcommand{\job}[5]{
  %% #1 start date 
  %% #2 end date
  %% #3 company name
  %% #4 role 
  %% #5 description 
  \begin{tabular}{l|l}
    #1 - #2 & #3 \\
  \end{tabular}
  \begin{quotation}
    \noindent\textit{#4}
  \end{quotation}
  \begin{quotation}
    \noindent#5
  \end{quotation} 
}

\newcommand{\project}[2]{
  %% #1 project name
  %% #2 description
  \begin{quotation}
    {\small\noindent\textit{#1}}
    \begin{quotation}
      {\small\noindent#2}
    \end{quotation}
  \end{quotation}
}

\newcommand{\pendingpatent}[2]{
  %% #1 patent #
  %% #2 description
  \begin{quotation}
    {\small\noindent#1 - #2}\\
    {\tiny Application pending.}
  \end{quotation}
}

\fancyhead[L]{
  \begin{tabular}{l}
    \textcolor{Mahogany}{\texttt{matt@tretin.net}}\\
    \textcolor{MidnightBlue}{203.247.0623}\\
    \\
    \\
  \end{tabular}
}

\fancyhead[C]{
  \begin{tabular}{c}
    \\
    \textcolor{Black}{\textbf{{\LARGE Matt Tretin}}}\\
    \\
    \\
  \end{tabular}
}

\fancyhead[R]{
  \begin{tabular}{r}
    \textcolor{Black!80}{123 East $82^{\mbox{nd}}$ St.}\\
    \textcolor{Black!80}{Apt 3D}\\
    \textcolor{Black!80}{New York NY, 10028}\\
    \\
  \end{tabular}
}

\begin{document}

\begin{quotation}
  \noindent I'm an experienced (10+ years) software engineer, systems architect,
  technical lead and engineering manager. I specialize in greenfield
  development, innovation, microservice architectures and online video
  distribution systems.
\end{quotation}

\noindent Patents:

\pendingpatent{USPTO 16/040,900}{Systems \& Methods for Securely Generating Live
  Previews}

\pendingpatent{USPTO 16/138,604}{Systems \& Methods for Generating
  Individualized Playlists}

\noindent Roles:
\vspace{.1in}

\job{Sept 2018}{Present}{Integral Ad Science}{OTT Team Lead}{Helping build the
  next generation of ad-measurement solutions.}

\job{Mar 2018}{Oct 2018}{FuboTV}{Research \& Innovation Engineer}{The Research \&
  Innovation team focused on rapidly implementing complex tactical projects and
  working with patent attorneys to document and pursue protections for core
  parts of FuboTV’s IP.}

\job{Oct 2016}{March 2018}{FuboTV}{Video Technical Lead \& Team Manager}{I was the
  first hire to start working on FuboTV’s OTT video ingest and distribution
  infrastructure. As the chief-architect and lead engineer I did initial rounds
  of coding myself over a frenzied rush of 16 hour days, eventually building out
  3 teams underneath me totaling 18 engineers and 7 direct reports. These
  systems support the international distribution of over 1,500 video streams to
  as many as 500,000 concurrent users, full cloud based DVR functionality, and
  support for SCTE224/35 switching for networks with a dynamic topology.}

\job{Jun 2015}{Aug 2016}{Perka}{Software Engineer}{I was part of the team
  responsible for maintenance and design of stable, performant, mission critical
  backend systems. My work focused on finding a safe way to migrate all
  production data from a custom relational data system built on top of Cassandra
  to PostgreSQL.}

\pagebreak 

\job{Jan 2015}{March 2015}{ESPN}{Data Platforms \& Architecture - Development
  Engineer}{Data Platforms \& Architecture group was responsible for the design,
  implementation, maintenance, and ongoing operation of production APIs
  leveraging a diverse set technologies meant to increase the efficacy of human
  users (butts-in-chairs) during the lifecycle of content creation and
  distribution both in-house and from remote locations.}

\job{Oct 2010}{Jan 2015}{ESPN}{Technology Innovation Team - Associate
  Development Engineer}{The Innovation Group's charter was to prototype a vision
  of how ESPN might look 3-5 years out by pushing current technologies to their
  absolute limit. Consider ESPN a factory for data about sports rather than a TV
  network. That data comes in many forms: 24x7 live-linear streams, clips,
  shows, web pages, database tables, etc. The innovation team devised new ways
  of connecting these disparate sources of data to create compelling end-user
  experiences and new capabilities for in house tooling.}

\job{Jan 2008}{Dec 2009}{Peak Systems}{Technician}{Provided on-site
  troubleshooting and repair for desktop PC systems (hardware), 3rd-party
  software, Microsoft Products, networking, etc. for secure data entry
  facilities on air-gapped networks.}

\job{Jan 2007}{Dec 2009}{University of Vermont}{Student Research
  Assistant}{Research in social networks, complex systems, and automated
  emotional classification of online text. We were the first team to measure the
  happiness of Twitter in real time. The team's work was covered in the New York
  Times and published as a letter to the journal Nature.}

\noindent Education:
\vspace{.1in}

\begin{tabular}{lll}
  2006 - 2010 & University of Vermont & Computer Science \& Philosophy\\
  2000 - 2004 & Weston High School &  \\
  2002 &  HCSSiM & \\
  2003 &  HCSSiM & \\ 
\end{tabular}

%% \project{Video Ad Live Preview Service}{I was responsible for the design,
%%   implementation, and ongoing operation of a custom video server enabling the
%%   marketing department to embed live (i.e. the World Cup game going on right
%%   now) video previews as both HTML5 ad pods and a custom embeddable widget for
%%   affiliate marketing sites. This lowered our average customer acquisition cost
%%   from the many tens of dollars to less than five. These campaigns converted
%%   impressions in record numbers and were the start of the funnel for more for
%%   than half of all new customers during the 2018 World Cup.}

%% \project{Video Stream Asset Twiddler}{I was responsible for the design and
%%   initial implementation of a custom video server providing HLS and DASH streams
%%   that break in predictable ways. This enabled test automation with consistent
%%   input and output expectations across multiple disparate platforms: Web, IOS,
%%   TvOS, Roku, Android, Chromecast, and FireTV. By creating tests with the same
%%   input and expected results for each platform, differences in player
%%   implementations, behaviors, and metric reporting became clearly identifiable
%%   and therefore fixable.}

%% \project{GEFF: Go Encapsulated FFMpeg}{I responsible for the design and
%%   implementation CGo library that encapsulated many common FFMpeg workflows,
%%   including PTS, DTS continuity analysis, muxing and demuxing of various program
%%   streams, PSNR analysis, etc. This was done by using the libav C libraries
%%   directly, not by wrapping the FFMpeg CLI enabling us to build efficient HTTP
%%   based microservers to manage these workflows.}

%% \project{Video Ingest}{A 24x7x365 ingest service responsible for the storage and
%%   time-wise indexing of over 1000 video streams for live distribution (about 14
%%   seconds behind cable)}

%% \project{Video Playlist Server}{A in-house HLS service providing both live and
%%   time-shifted access to video content. This service is regularly (automated)
%%   load tested to more than 500,000 concurrent users and has seen in upwards of
%%   100,000 concurrent users in production}

%% \project{SCTE224 \& MIMS (Manual Ingest Mapping System)}{FuboTV had the first
%%   commercially available SCTE224 data system for integrating with the MCRs
%%   (master control rooms) of broadcast partners. Basically, this system allows
%%   broadcasters like Fox and NBC to define which stations (i.e. local Fox in NY
%%   is not the same channel, content, incorporation, etc. as the local Fox in
%%   Pittsburg) receive which video streams by seamlessly integrating into their
%%   current production workflows in real time. For broadcast partners that did not
%%   yet have this technology available, we created a manual system for our
%%   programming and content teams to use.}

%% \project{A New Architecture}{I was a principal architect helping to design a
%%   top-to-bottom web-stack built from well known open source solutions meant to
%%   replace an ailing custom codebase. The system we were replacing consisted of a
%%   database, web server, and an event dispatch/event handling mechanism -- all of
%%   which were custom built. The system was full of very ugly patches, leaky
%%   abstraction, and was difficult to modify. Switching to battle-tested open
%%   source solutions allowed us to eliminate 72\% of the current codebase without
%%   compromising any functionality. In addition, we decreased the number of AWS
%%   instances needed to run a complete stack by an order of magnitude. In order to
%%   migrate the data, raw data blobs from the custom database needed to be
%%   translated to a properly relational set of SQL records and tables. I was the
%%   sole engineer tasked with writing this migration utility. The biggest
%%   challenges were translating an object graph where logical equality was
%%   sufficient to object graphs where object equality was required, de-tabling
%%   almost four years of object-reference rot, and be performant enough to crunch
%%   just over 100GB of data in under twenty minutes}

%% \project{Umpire Strike Zone Visualizer}{The Innovation Group was asked to
%%   prototype a visualization tool allowing a user to compare the “average”
%%   strike-zone of any two major league umpires across an arbitrary dimension from
%%   a predefined set, i.e. the same the same player, or team, etc. The
%%   visualization showed a heat-map of an MxN grid representing a region larger
%%   than the strike zone. Cells in the grid where colored to show whether more
%%   pitches in that cell were balls or strikes, showing the derived strike zone as
%%   a gradient and the defined strike zone as a superimposed box}

%% \project{TV2016}{In order to understand how our product (a cable TV network)
%%   could be adapted to leverage at-home IP video distribution technologies, the
%%   Innovation Team was tasked with inventing the “cable box” for an IP-multicast
%%   based TV distribution network. The system consisted of an automated clipping
%%   and tagging framework that would cut up every single MLB game and episode of
%%   SportsCenter into each component play or segment and add most relevant
%%   metadata (i.e. what teams/people, what happens, etc.). The client displayed
%%   all this on a custom built iPad application that communicated with an Android
%%   app running on a GoogleTV allowing users to navigate through all the content
%%   (live) and choose where to watch it (iPad, any of several “cable boxes”/TVs).}

%% \project{MrEncoder / MrRest / NASAmatic}{I was the sole engineer implementing
%%   a production quality version of the custom HLS video server that powered
%%   TV2016 (above). This meant writing a Java HLS library capable of speaking all
%%   major versions of the HLS protocol and representing all the different metadata
%%   elements described by HLS (timing info, closed captions, etc.). This system
%%   was used to publish college sports clips by owning them through the process of
%%   ingestion (go from a satellite dish to something we can use), clipping (what
%%   part do I want?), tagging (who’s/what’s in it?), all the way to distribution
%%   (how do we get it to a CDN?). Using this system to clip and tag instead of
%%   expensive “edit seats” connected to a industry standard storage system
%%   (Quantel) dramatically increased the productivity of individuals: the
%%   “automatic” clipping capabilities were augmented to make the people
%%   responsible for clipping and tagging more accurate and more productive. In
%%   order to handle a special request from NASA, were asked to adapt this system
%%   to send SportsCenter, Around The Horn, and PTI, to the International Space
%%   Station}

%% \project{Semantic Databases}{I was the sole engineer trying to solve the
%%   problem: When one DB powers graphics systems, one DB powers espn.com, and one
%%   DB powers the statistical analysts, etc., and each database has its own unique
%%   ID for a common external entity (i.e. Michael Jordan had N different integer
%%   IDs across the company) , how can we tag content with entities in one system
%%   and understand them in the context another? In other words, I was tasked with
%%   figuring how to create a unique notion of identity across several data systems
%%   all describing the same world in slightly different ways.  I got as far as a
%%   proof-of-concept that could translate primary keys based on computing the
%%   similarity of “identity-facts” defined in terms of a common ontology about
%%   each entity in each database and doing a fuzzy-comparison.}


\end{document} 


